# -*- coding: utf-8 -*-
"""Random_forestModel(xy0628-01Code).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SCUFUZMgBZ5rayo4p10eOSvckyJAklI8
"""

# Installing Necessary Libraries and Requirements
import pandas as pd
import matplotlib.pyplot as plt

"""# Loading and Exploring the Dataset"""

data = pd.read_csv('Data(2).csv')

data.head()

data.tail()

"""## 1. Amount Spend On Items Sold: BoxPlot Visualization"""

# Create a box plot to show the amount spent on each item sold
plt.figure(figsize=(10, 6))
data.boxplot(column='Amount', by='Category', patch_artist=True, boxprops=dict(facecolor='lightblue'))
plt.xlabel('Category')
plt.ylabel('Amount')
plt.title('Amount Spent on Each Category')
plt.xticks(rotation=90)
plt.show()

"""# Model Building

## Data Preparation
-   First Off All; The Data Has Categorical Variables. <br>
-   One-Hot Encoding was Done to Convert the Categorical Column Variables into Suitable Values That Could work with Model Building libraries such as `sklearn`
-   Next, the Encoded data Is split Into Target Variable `Status_cancelled` and Input Features.
"""

#Requirements
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split

# Specify the categorical columns that need to be one-hot encoded
categorical_columns = ['Status', 'Fulfilment', 'Delivery',
                       'Size', 'State', 'Promotion',
                       'Category', 'Month']

# Perform one-hot encoding on the categorical columns
encoded_data = pd.get_dummies(data, columns=categorical_columns)

# Display the encoded data
print(encoded_data.head())
print(encoded_data.tail())

# split Into Target Variable Status_cancelled and Input Features.
X = encoded_data.drop('Status_Cancelled', axis=1)
y = encoded_data['Status_Cancelled']

"""# Training and Testing Split
The Data Is Split Into a Training (25%) and Testing (75%) Set
"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

"""## The Random Forest Model
*   We Use SimpleImputer to Replace All Missing Values with a Mean
"""

## Using Imputer to Take Care of missing Values..
from sklearn.impute import SimpleImputer

# Create an instance of SimpleImputer with the desired strategy
imputer = SimpleImputer(strategy='mean')

# Fit the imputer on the training data
imputer.fit(X_train)

# Transform the training and testing data using the fitted imputer
X_train_imputed = imputer.transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Decision Tree Model Instance Creation, Building and Fitting
# Creating a Confusion Matrix
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train_imputed, y_train)
dt_predictions = dt_model.predict(X_test_imputed)
dt_cm = confusion_matrix(y_test, dt_predictions)

# Resimulate Fitting Model On Training Data
dt_model.fit(X_train_imputed, y_train)

"""## Random Forest Model
-   We Create and Train the Random Forest Model Using the Same Imputed Data as the Decision Tree
"""

# Instanciating
rf_model = RandomForestClassifier(random_state=42)
# Fitting the model data
rf_model.fit(X_train_imputed, y_train)

# Predicting on the test Data
rf_predictions = rf_model.predict(X_test_imputed)

# Creating a Confusion Matrix
rf_cm = confusion_matrix(y_test, rf_predictions)

"""## Accuracy Scores Calculations
We Calculate the Accuracy Scores for Both Models Above
"""

dt_accuracy = accuracy_score(y_test, dt_predictions)
rf_accuracy = accuracy_score(y_test, rf_predictions)

"""# Results Displayed
-  The Results of Both Models are As Displayed Below...
"""

print("Decision Tree Confusion Matrix:")
print(dt_cm)
print("Decision Tree Accuracy Score:", dt_accuracy)
print()
print("Random Forest Confusion Matrix:")
print(rf_cm)
print("Random Forest Accuracy Score:", rf_accuracy)